1. How do you execute jobs parallely in Jenkins?
Ans:-
In Jenkins, you can execute jobs in parallel using several methods, depending on your requirements and Jenkins setup. Here are a few common approaches:
    1. Pipeline Parallelism: If you are using Jenkins Pipeline, you can leverage the parallel directive to execute stages or tasks concurrently. Here's a basic example:
example code :-

pipeline {
    agent any
    stages {
        stage('Parallel Stage') {
            parallel {
                stage('Task A') {
                    steps {
                        // Execute task A
                    }
                }
                stage('Task B') {
                    steps {
                        // Execute task B
                    }
                }
            }
        }
    }
}


2. Maven Lifecycle ?
ans:-
Maven Lifecycle consists of three main lifecycles: default, clean, and site.

Default Lifecycle:
The default lifecycle handles the project deployment, compilation, testing, packaging, and deployment. It consists of the following phases:

validate: Validates the project is correct and all necessary information is available.
compile: Compiles the source code of the project.
test: Tests the compiled source code using a suitable testing framework.
package: Packages the compiled code into a distributable format such as JAR or WAR.
install: Installs the packaged artifact into the local repository, making it available for other projects locally.
deploy: Copies the final package to the remote repository for sharing with other developers or projects.

Clean Lifecycle:
The clean lifecycle handles project cleaning, removing files generated by the previous build. It consists of the following phases:

pre-clean: Executes tasks before the cleaning process starts.
clean: Removes all files generated by the previous build.
post-clean: Executes tasks after the cleaning process completes.

Site Lifecycle:
The site lifecycle generates project documentation and reports. It consists of the following phases:

pre-site: Executes tasks before the site generation starts.
site: Generates project documentation and site.
post-site: Executes tasks after the site generation completes.
site-deploy: Deploys generated site documentation to a web server or repository.


3. What is called a Parameterised Job in Jenkins ?
ans:-
Parameterized jobs are useful when you want to reuse the same job configuration with different inputs or configurations.

Common types of parameters in Jenkins include:

String Parameter: Allows you to specify a string value as a parameter.

Boolean Parameter: Represents a true/false value.

Choice Parameter: Provides a dropdown menu of predefined choices.

File Parameter: Allows you to specify a file as a parameter.

Password Parameter: Allows you to specify a password parameter that is masked in the Jenkins UI.

Multi-line String Parameter: Allows you to specify a multi-line string as a parameter.


4. Can you use Multiple FROM in DockerFile
ans:- 
In a Dockerfile, you can only have one FROM instruction, and it must be the first non-comment instruction. This is because the FROM instruction specifies the base image from which you're building your Docker image, and Docker images are built in layers. Each FROM instruction starts a new build stage and effectively resets the build process.

However, you can use multiple FROM instructions in a Dockerfile if you're using a feature called multi-stage builds, introduced in Docker 17.05 and later. Multi-stage builds allow you to use multiple FROM instructions to create intermediate images and copy artifacts between them, without including unnecessary dependencies or build tools in the final image.

Here's an example of how you can use multi-stage builds in a Dockerfile:
# Stage 1: Build stage
FROM maven:3.8.1-openjdk-11 AS build
WORKDIR /app
COPY . .
RUN mvn clean package

# Stage 2: Production stage
FROM openjdk:11-jre-slim
WORKDIR /app
COPY --from=build /app/target/your-app.jar /app/your-app.jar
EXPOSE 8080
CMD ["java", "-jar", "/app/your-app.jar"]

Explanation
    1. Stage 1: Build Stage
        ◦ Base Image: maven:3.8.1-openjdk-11 - This is the Maven image with OpenJDK 11, which is used to build the Java application.
        ◦ WORKDIR: /app - The working directory inside the container.
        ◦ COPY: COPY . . - Copies all the files from the current directory on your host machine to the /app directory in the container.
        ◦ RUN mvn clean package: This command runs Maven to clean the project and package it into a JAR file.
    2. Stage 2: Production Stage
        ◦ Base Image: openjdk:11-jre-slim - This is a slim image of OpenJDK 11 JRE, suitable for running the application.
        ◦ WORKDIR: /app - The working directory inside the container.
        ◦ COPY --from=build /app/target/your-app.jar /app/your-app.jar: Copies the packaged JAR file from the build stage to the production stage.
        ◦ EXPOSE 8080: Exposes port 8080 to be accessible from outside the container.
        ◦ CMD ["java", "-jar", "/app/your-app.jar"]: The command to run the Java application.
Note
    • Ensure the COPY --from=build /app/target/your-app.jar /app/your-app.jar line matches the actual path and name of your built JAR file.
    • The Maven build stage assumes your project will produce a JAR file named your-app.jar in the target directory. Adjust this according to your actual project setup.
This multi-stage Dockerfile builds the Java application in one stage and then copies the built artifact into a lightweight runtime image in the second stage, ensuring a smaller and more efficient production image.






5. DockerFile runs as which user ?
ans:-
By default, when you build and run a Docker container, the processes inside the container run as the root user. This behavior is inherited from the host machine's user namespace.

However, it's important to note that running processes as the root user inside a container can pose security risks, as any compromise of the container could potentially lead to the compromise of the entire host system.

To mitigate these risks, it's recommended to follow the principle of least privilege and run processes inside the container as a non-root user whenever possible.

You can specify a non-root user in your Dockerfile using the USER instruction. For example:

Dockerfile
FROM some-base-image

# Create a new user
RUN groupadd -r myuser && useradd -r -g myuser myuser

# Set the working directory
WORKDIR /app

# Copy application files
COPY . .

# Change the ownership of the application directory to the newly created user
RUN chown -R myuser:myuser /app

# Switch to the newly created user
USER myuser

# Run your application
CMD ["./myapp"]
In this example:

We create a new user and group (myuser) using the groupadd and useradd commands.
We set the working directory to /app and copy the application files into it.
We change the ownership of the /app directory to the myuser user.
Finally, we switch to the myuser user using the USER instruction and specify the command to run the application.
By running processes inside the container as a non-root user, you can reduce the potential impact of security vulnerabilities.

6. How can we pass an argument to DockerFile ?
ans:-
TO pass an argument to dockerfile we use ARG command
ex:- ARG my_arg=value




7. What are deployment strategies ?
ans:-


8. What are Register targets in Ansible?
ans:- 
In Ansible, "register" is a feature that allows you to capture the output of a task and save it to a variable. 
ex:- - name: diplay index.html
      command: "cat /home/rahul.yadav@apmosys.mahape/sachin/index.html"
      register: output
    - debug: var=output      

    
9. How do you uncommit the changes that have already been pushed to GitHub?    
ans:- by using git revert or git reset


10. What is the difference between git pull and git fetch?
Ans:-
difference between git pull & git fetch is git fetch, git fetch only downloads the changes from the remote repository to your local repository. It updates your local copy of the remote branches, but it does not automatically merge or rebase those changes into your current branc while git pull is git pull combines two operations: git fetch followed by git merge or git rebase. It downloads the changes from the remote repository to your local repository and then automatically merges or rebases those changes into your current branch.

11. What is called Jenkins File?
ans:- 
A Jenkinsfile is a text file that contains the definition of a Jenkins Pipeline. Jenkins Pipeline is a suite of plugins that supports continuous integration (CI) and continuous delivery (CD) automation. It allows you to define a series of steps or stages to orchestrate the build, test, and deployment process of your software application.

The Jenkinsfile is typically stored in the version control repository alongside your application code. It defines the entire build pipeline as code, enabling you to manage and version control your build process along with your application code. This approach brings several benefits, including:

Version Control: The Jenkinsfile is versioned along with your application code, providing a complete history of changes to your build process over time.
Reproducibility: With the build process defined as code in a Jenkinsfile, builds are reproducible and can be executed consistently across different environments.
Visibility: The Jenkinsfile provides visibility into the entire build process, including the sequence of steps, dependencies, and conditions for executing each step.
Flexibility: Jenkinsfiles support advanced features such as parallel execution, error handling, and conditional logic, allowing you to create complex build pipelines tailored to your specific requirements.
Portability: Since the build process is defined as code, it can be easily shared and reused across different projects and teams.
Jenkinsfiles are typically written using the Declarative Pipeline syntax or the Scripted Pipeline syntax. Declarative Pipeline offers a more structured and concise syntax for defining pipelines, while Scripted Pipeline provides more flexibility and control with a Groovy-based scripting language.

In summary, a Jenkinsfile is a crucial component of Jenkins Pipeline, defining the entire build, test, and deployment process of your software application as code.


12. What is called Shared Libraries in Jenkins?
ans:- 
In Jenkins, Shared Libraries are a way to modularize and share code across multiple pipelines. They allow you to define reusable code, such as custom steps, functions, or entire pipeline workflows, in a centralized location, making it easier to manage and maintain complex pipelines across different projects or teams.

Shared Libraries are typically organized as Git repositories containing Groovy scripts, along with any other resources or dependencies needed by the scripts. These repositories can be stored locally or on remote version control systems like GitHub or Bitbucket.

Key features and benefits of Shared Libraries in Jenkins include:

Code Reusability: Shared Libraries promote code reuse by allowing you to define common functionality once and share it across multiple pipelines.
Centralization: Shared Libraries centralize the management of pipeline code, making it easier to maintain and update shared functionality.
Version Control: Shared Libraries are stored in version control repositories, enabling versioning, change tracking, and collaboration among team members.
Customization: Shared Libraries can be customized and extended to meet the specific requirements of your organization or projects.
Isolation: Shared Libraries run in an isolated environment, ensuring that changes made to one pipeline do not affect other pipelines that use the same library.
Security: Shared Libraries support access control and permissions, allowing you to restrict access to sensitive pipeline code and configurations.
To use a Shared Library in a Jenkins pipeline, you typically define a reference to the library in your Jenkinsfile and then call functions or methods defined in the library. Jenkins automatically loads the Shared Library code and makes it available to your pipeline at runtime.

Overall, Shared Libraries in Jenkins provide a powerful mechanism for promoting code reuse, improving maintainability, and streamlining the development and deployment of complex pipeline workflows.

13. What is called docker networking?
ans:- 
Docker networking refers to the networking capabilities and features provided by Docker to enable communication between Docker containers, as well as between containers and other networked resources such as the host system or external networks.

Docker provides several networking options that allow you to configure how containers communicate with each other and with the outside world. Some of the key aspects of Docker networking include:

Bridge Networking: Docker creates a bridge network by default when you install it. Each container on the bridge network gets its own IP address, allowing containers to communicate with each other and with the host system. Bridge networking is suitable for most use cases and is the default networking mode for Docker containers.
Host Networking: With host networking, Docker containers share the network namespace with the host system, effectively giving them access to the host's network interfaces. This can improve network performance but may limit the isolation between the containers and the host.
Overlay Networking: Overlay networking allows containers running on different Docker hosts to communicate with each other securely over an overlay network. This is useful for deploying applications across multiple hosts or in a distributed environment, such as a cluster or swarm.
Macvlan Networking: Macvlan networking allows you to assign MAC addresses to containers, making them appear as physical devices on the network. This can be useful for scenarios where containers need direct access to the network, such as when running network services or accessing hardware devices.
User-defined Networks: Docker allows you to create custom networks with specific configurations, such as subnet ranges, gateway addresses, and DNS settings. This gives you more control over how containers are connected and configured within the network.
Container-to-Container Communication: Docker containers can communicate with each other using standard networking protocols such as TCP/IP or UDP. You can use container names, IP addresses, or DNS aliases to reference other containers within the same network.
External Connectivity: Docker containers can also communicate with external networks, such as the internet or other networked resources, by configuring port mappings or exposing network services to the host system.
Overall, Docker networking provides a flexible and powerful way to connect and manage communication between containers, making it easier to deploy and manage containerized applications in various environments.











14. What is called Public Subnet and Private Subnet?
ans:- 
Public Subnet:
A public subnet is a subnet within a network that is accessible from the public internet. It typically contains resources or services that need to be directly accessible from the internet, such as web servers, email servers, or public-facing applications. Public subnets are assigned public IP addresses that can be reached from anywhere on the internet. These IP addresses are routable and can be accessed by devices outside the local network.

Private Subnet:
A private subnet, on the other hand, is a subnet within a network that is not directly accessible from the public internet. It typically contains resources or services that are intended to be accessed only from within the local network or a trusted network environment, such as databases, internal applications, or backend services. Private subnets are assigned private IP addresses that are not routable on the public internet. Instead, they rely on network address translation (NAT) or other mechanisms to access resources outside the local network.

In summary, the main difference between a public subnet and a private subnet lies in their accessibility from the internet. Public subnets are accessible from the internet and are assigned public IP addresses, while private subnets are not directly accessible from the internet and are assigned private IP addresses. 

15. How do you install Nginx in the Ansible playbook?
ans:-

---
- name: Installing  nginx
  hosts: all
  become: yes
  tasks: 
    - name: Install nginx
        apt:
          name: nginx
          state: present
          
          
16. What is called “FROM SCRATCH” in Docker?
ans:- 

In Docker, FROM scratch is a special instruction used in Dockerfiles to create a minimalist container image from scratch, without using any pre-built base image. When you use FROM scratch, you're essentially starting with an empty container filesystem and building up from there, adding only the files and resources necessary for your application to run.









17. Can we run the container inside the container?
ans:- 
Yes, it is possible to run a container inside another container, a practice known as "Docker-in-Docker" (DinD). This approach can be useful in various scenarios, such as CI/CD pipelines where you might need to build and test Docker images within a containerized environment.

To achieve this, you generally have two main approaches:

Docker-in-Docker (DinD) using a Docker image:

Docker provides a specific image called docker:docker that is designed for running Docker inside Docker. You can start a container using this image, which will have the Docker engine running inside it.




Example usage:



docker run --privileged --name docker-in-docker -d docker:docker
The --privileged flag is necessary because Docker needs additional permissions to run properly inside another Docker container.
Bind Mounting the Docker Socket:

Instead of running Docker inside a container, you can bind mount the host machine’s Docker socket into your container. This allows the container to communicate with the Docker daemon of the host system, effectively giving the container Docker capabilities.
Example usage:

docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker my-docker-client-image
This method is generally preferred over DinD for most use cases because it avoids the complexities and potential security risks associated with running a full Docker daemon inside a container.


18. What is called Persistent Storage in Docker?
ans:-
Persistent storage in Docker refers to the storage solutions that ensure data created by a container is saved and available even after the container is stopped, removed, or recreated. This is essential for stateful applications where data needs to be preserved between container lifecycles. There are two main methods to achieve persistent storage in Docker: Volumes and Bind Mounts.

Volumes
Volumes are the preferred mechanism for persisting data in Docker. They are managed by Docker and offer several advantages, including portability, flexibility, and ease of use. Volumes can be stored on the host filesystem but are managed through Docker commands, making them less dependent on the host OS and more secure.

Creating and Using Volumes
Creating a Volume:

docker volume create my_volume
Using a Volume in a Container:


docker run -d --name my_container -v my_volume:/path/in/container my_image

Bind Mounts

Bind mounts are a way to mount a specific directory or file from the host filesystem into a container. Unlike volumes, bind mounts rely directly on the directory structure and permissions of the host OS.

Using Bind Mounts
Running a Container with a Bind Mount:


docker run -d --name my_container -v /path/on/host:/path/in/container my_image
Comparison: Volumes vs. Bind Mounts
Volumes:

Managed by Docker, offering better portability and isolation from the host.
Typically stored in Docker's managed directory (/var/lib/docker/volumes/ on Linux).
Useful for production environments due to better security and backup options.
Bind Mounts:

Directly map a host directory to a container directory.
Useful for development environments where you need direct access to host files.
Less portable and more dependent on the host filesystem structure.
Example Scenarios
Using Volumes for a Database
To run a MySQL container with persistent storage using a volume:


docker volume create mysql_data
docker run -d --name mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -v mysql_data:/var/lib/mysql mysql:latest
Using Bind Mounts for Development
To run a web server container that serves files from a host directory:


docker run -d --name webserver -v /path/to/website:/usr/share/nginx/html:ro -p 8080:80 nginx:latest


19. What happens when you delete /var/lib/docker/overlay ?
ans:- 
Deleting the /var/lib/docker/overlay or /var/lib/docker/overlay2 directory will result in the loss of all Docker images, containers, and data, causing Docker to become non-functional. This action should be avoided to prevent severe disruptions and data loss. Regular backups and using Docker volumes for important data can help mitigate these risks.



20. What are called regular expressions in Linux?
ans:- 
Regular expressions in Linux, often abbreviated as regex, are sequences of characters that define a search pattern. They are used for string matching and manipulation in various tools and programming languages. Common uses include searching, replacing, and extracting text in utilities like grep, sed, and awk.

Examples:

^abc: Matches any line starting with "abc".
abc$: Matches any line ending with "abc".
a.b: Matches any string containing "a" followed by any character and then "b".
[a-z]: Matches any lowercase letter.
Regular expressions are powerful tools for text processing and pattern matching in Linux.


21. How do you push the image to DockerHub?
ans:- 
docker login
docker tag myimage:latest myusername/myrepository:latest
docker push myusername/myrepository:latest

22. What is the difference between CMD and Entrypoint in Docker?
ans:-
CMD: Specifies the default command to run when starting a container, but can be overridden by arguments provided in docker run.
ENTRYPOINT: Defines the executable that will always run when the container starts, and can optionally use CMD to provide default arguments that can be overridden.
In summary, CMD is more flexible for providing defaults, while ENTRYPOINT is used to set a fixed command.

23. How does Ansible execute the jobs?
ans:-
Ansible executes jobs by connecting to remote nodes over SSH (by default), transferring modules to those nodes, and then executing the tasks defined in the playbook in a sequential manner.

24. What is a GIT tag?
ans:- 
A Git tag is a named pointer to a specific commit in a Git repository. It is typically used to mark significant points in the commit history, such as software releases or milestones, making it easier to reference and retrieve specific versions of the codebase.

25. How do you configure ansible in Jenkins?
ans:- 
To configure Ansible in Jenkins:

Install the Ansible plugin in Jenkins.
Set up Jenkins credentials to securely store the SSH private key or username/password for connecting to remote servers.
Configure a Jenkins job (Freestyle or Pipeline) and add a build step to execute Ansible playbooks or commands.
Provide the necessary playbook file path, inventory file, and any additional options.
Save the job configuration and run the job to execute Ansible tasks on your target servers.

26. Difference between an ant and a maven?
ans:- 
Ant and Maven are both build automation tools used in Java development, but they have different approaches:

Ant: Uses XML-based configuration files and allows fine-grained control over build processes. It's more flexible but requires manual management of dependencies and build scripts.
Maven: Uses convention over configuration and standardizes project structure. It manages dependencies automatically and provides built-in lifecycle management and dependency resolution.





27. Git workflow?
ans:- 
Git workflow refers to a set of rules or guidelines for how developers collaborate on a Git repository. Common Git workflows include centralized, feature branch, Gitflow, and GitHub flow, each with its own approach to branching, merging, and releasing code.


28. Where do you find errors in Jenkins?
ans:- 
Errors in Jenkins can be found in several places:

Console Output: Directly in the Jenkins job's console output, which displays the logs of each build step. Errors and warnings are typically highlighted here.

Build History: Errors are indicated by a red ball icon in the Jenkins build history. Clicking on a failed build will show details of the error.

Build Logs: Jenkins stores build logs in the Jenkins home directory. You can navigate to the specific build's directory and view the logs directly.

System Log: Jenkins maintains a system log where it records events, errors, and warnings related to the Jenkins server itself. This log can be accessed from the Jenkins web interface or directly from the filesystem.

29. Differences between git rebase and git merge?
ans:-
git rebase:

Rewrites the commit history by moving the entire feature branch onto the tip of the main branch.
Results in a linear history with a cleaner, more straightforward timeline.
Recommended for feature branches with a short lifespan and small, frequent changes.

git merge:

Combines changes from one branch into another, creating a new merge commit.
Preserves the commit history of both branches, maintaining a more complex, but accurate representation of the development process.
Suitable for integrating long-lived feature branches or when preserving the context of individual commits is important.


30. What is git init?
ans:-
git init is a command used to initialize a new Git repository in a directory. When you run git init in a folder, it sets up all the necessary files and directories that Git needs to start tracking changes and managing versions of your files within that directory. After running git init, the directory becomes a Git repository, and you can start adding files, committing changes, and utilizing Git's version
 control features.




31. What is a git clone?
ans:-
git clone is a command used to create a copy of an existing Git repository from a remote source. It downloads all the files and commit history from the remote repository to your local machine, allowing you to work on the project locally and collaborate with others

32. If there is suddenly the file is deleted in git how do you get it back?
ans:-
To restore a deleted file in Git:

Use git checkout with the commit hash or branch name where the file existed before deletion.
Alternatively, you can use git restore or git revert commands to retrieve the deleted file from a previous commit.

33. What is the purpose of Docker?
ans:-
The purpose of Docker is to simplify the process of creating, deploying, and managing applications by using containerization technology. Docker allows developers to package their applications and all their dependencies into a standardized unit called a container, ensuring consistency across different environments and making it easy to deploy applications on any infrastructure. It provides a lightweight, portable, and efficient way to build, ship, and run applications in various environments, from development to production.

34. In Jenkins how can you find log files?
ans:-
In Jenkins, you can find log files:

By navigating to the Jenkins home directory on the filesystem and accessing the logs directory.
Through the Jenkins web interface by clicking on "Manage Jenkins" > "System Log" to view system-wide logs, or by clicking on a specific job's build history to view its console output.

35. What is the use of ansible?
ans:-
Ansible is used for automating IT infrastructure tasks such as provisioning, configuration management, application deployment, and orchestration. It simplifies complex tasks by allowing users to describe infrastructure as code using YAML-based playbooks, enabling consistent and repeatable deployment processes across environments.

36. What is configuration management?
ans:-
Configuration management is the process of systematically managing and controlling changes to a system's configuration, ensuring that it remains consistent, accurate, and up-to-date over time. It involves identifying, documenting, and tracking configuration items, as well as implementing procedures to maintain and update them as needed. Configuration management is essential for ensuring the reliability, stability, and security of IT infrastructure and software systems.







37. Jenkins workflow and write a script for this workflow ?
ans:-
Jenkins Workflow is a feature that allows the creation of complex, multi-step build processes using a Groovy-based DSL (Domain-Specific Language). It provides a way to define build pipelines, which can include parallel or sequential stages, conditional logic, and integrations with external systems.

A simple Jenkins Workflow script might look like this:

pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                // Build the project (e.g., compile code)
                sh 'mvn clean install'
            }
        }
        stage('Test') {
            steps {
                // Run tests
                sh 'mvn test'
            }
        }
        stage('Deploy') {
            when {
                // Deploy only if the build and test stages were successful
                expression {
                    currentBuild.result == 'SUCCESS'
                }
            }
            steps {
                // Deploy the application
                sh 'ansible-playbook deploy.yml'
            }
        }
    }
}






38. In Ubuntu sever what is a public key and private key ?
ans:-
In Ubuntu server, a public key and private key are cryptographic keys used for secure communication and authentication. The public key is shared with others and used to encrypt data or verify signatures, while the private key is kept secret and used to decrypt data or create signatures. Together, they form a key pair that enables secure communication and authentication in various protocols such as SSH, SSL/TLS, and PGP.

39. Three members have the same password one I have to change write a script for this example ?
ans:-
#!/bin/bash

# Define the usernames
user1="username1"
user2="username2"
user3="username3"

# Define the new password for the user to be changed
new_password="new_password"

# Change the password for user1
echo "$user1:$new_password" | sudo chpasswd

# Display the new password for user1
echo "Password changed for $user1. New password: $new_password"


40. Difference between SVN and GIT?
ans:-
SVN (Subversion):

Centralized version control system.
Single repository for all files and history.
Requires network access to the central server for most operations.
Branching and merging can be more complex and slower.
Git:

Distributed version control system.
Each user has a full local copy of the repository.
Many operations can be performed offline.
Branching and merging are more efficient and faster.

41. git commit ?
ans:-
git commit is a command used to save changes to the local repository. It creates a snapshot of the current state of the working directory, including a log message that describes the changes. This snapshot can then be pushed to a remote repository for sharing with others.




42. Git push and fetch?
ans:-
git push: Uploads local commits to a remote repository, updating the remote branch with your latest changes.
git fetch: Downloads commits, files, and references from a remote repository into your local repository without merging them into your working branch.


43. Difference between Docker and Ansible?
ans:-
Docker is a containerization platform that allows you to package, distribute, and run applications within isolated containers, while Ansible is a configuration management tool used for automating provisioning, deployment, and orchestration of IT infrastructure and applications. In summary, Docker focuses on containerization and runtime environment isolation, while Ansible focuses on automation and configuration management of infrastructure and software deployments.

44. Difference between rebasing and merging?
ans:-
The main difference between git merge and git rebase is that git merge is a way of combining changes from one branch (source branch) into another branch (target branch) where as git rebase is a way of moving the changes from one branch onto another branch.

45. Jenkins workflow ?
ans:-
Jenkins Workflow is a feature that allows the creation of complex, multi-step build processes using a Groovy-based DSL (Domain-Specific Language). It provides a way to define build pipelines, which can include parallel or sequential stages, conditional logic, and integrations with external systems.

46. How to build a job in Jenkins by using git and maven?
ans:-
To build a job in Jenkins using Git and Maven:

Create a New Job:

Go to the Jenkins dashboard.
Click "New Item".
Enter a name for the job and select "Freestyle project".
Click "OK".
Configure Source Code Management:

Under "Source Code Management", select "Git".
Enter the repository URL and credentials if needed.
Specify the branch to build.
Configure Build Steps:

Under "Build", click "Add build step" and select "Invoke top-level Maven targets".
Specify the Maven goals (e.g., clean install) and options.
Save and Run:

Click "Save" to save the job configuration.
Click "Build Now" to manually trigger a build.
This will build the Maven project from the Git repository specified in the job configuration.

47. What is the use of maven in Jenkins?
ans:-
The use of Maven in Jenkins is to automate the build process of Java projects. Maven is a build automation tool that manages project dependencies, compiles source code, executes tests, and packages the application into distributable formats. Integrating Maven with Jenkins allows for automated and repeatable builds of Java projects, streamlining the development and deployment process.

48. What are the modules you have used in Ansible?
ans:-
Some common modules used in Ansible include:

apt / yum: For package management on Debian/Ubuntu or Red Hat/CentOS systems.
copy / template: For copying files or templates to remote hosts.
service: For managing services on remote hosts (e.g., starting, stopping, restarting services).
file: For managing files and directories on remote hosts (e.g., creating, deleting, setting permissions).
shell / command: For running commands or shell scripts on remote hosts.
git / svn: For managing Git or Subversion repositories.
template: For using Jinja2 templates to generate files dynamically.
debug: For printing debug messages during playbook execution.

49. Docker purpose and usage?
ans:-
Docker is used for containerization, which involves packaging software and its dependencies into standardized units called containers. Its purpose is to simplify the process of developing, deploying, and running applications by providing a consistent environment across different systems. Docker containers can be used to isolate applications, manage dependencies, improve scalability, and streamline deployment processes.

50. The flow of SonarQube? why do we use it?
ans:-
The flow of SonarQube involves analyzing code for quality and security issues, providing actionable feedback to developers, and facilitating continuous improvement of code quality. SonarQube is used to ensure code maintainability, reliability, and security by identifying bugs, vulnerabilities, and code smells early in the development process. It helps teams deliver higher-quality software by enforcing coding standards, promoting best practices, and enabling informed decisions based on comprehensive code analysis.

51. What is the use of quality gates in sonarqube?
ans:-
The use of quality gates in SonarQube is to set thresholds for code quality metrics such as code coverage, code duplication, and code smells. Quality gates define conditions that must be met for code to pass inspection and be considered acceptable. They serve as checkpoints in the software development process, ensuring that only high-quality code is promoted to subsequent stages or released to production. Quality gates help teams enforce coding standards, maintain code quality, and prevent the introduction of new issues into the codebase.










52. Suppose we give 30 % quality I want you to define in quality gates ?
ans:-
In quality gates, setting a 30% threshold means that the code must meet certain quality criteria defined by metrics such as code coverage, code duplication, and code smells. For example, if code coverage must be at least 30% for the gate to pass, it indicates that at least 30% of the codebase is covered by automated tests. Similarly, other metrics should also meet the specified thresholds for the overall quality gate to pass, ensuring that the code meets minimum quality standards before further development or deployment.

53. As a DevOps engineer why do we use Jira Tool?
ans:-
As a DevOps engineer, we use Jira tool for issue tracking, project management, and collaboration. It helps us manage tasks, track progress, and coordinate activities across development, operations, and other teams involved in the software delivery lifecycle. Jira provides visibility into project status, facilitates communication, and enables efficient workflow management, ultimately contributing to faster and more reliable software delivery.

54. Why do we use a pipeline in Jenkins? Flow?
ans:-
We use a pipeline in Jenkins to automate and orchestrate the software delivery process, from code commit to deployment. The pipeline defines the flow of tasks, including building, testing, and deploying applications, enabling continuous integration and delivery. By using a pipeline, we can ensure consistency, repeatability, and traceability in the software delivery process, leading to faster release cycles and higher-quality software.

55. What is Release management due to production?
ans:-
Release management to production involves the planning, coordination, and execution of activities required to deploy software releases into production environments. It encompasses tasks such as scheduling releases, coordinating with stakeholders, preparing deployment plans, performing deployment activities, and monitoring the release process. The goal of release management is to ensure smooth and reliable deployments of software changes into production, minimizing downtime and impact on users while maximizing the value delivered to customers.

56. What is SSL? And how does it work internally?
ans:-
SSL (Secure Sockets Layer) is a protocol used to secure communication over a computer network, typically between a web browser and a web server. It works by encrypting data transmitted between the client and server, ensuring that it remains confidential and cannot be intercepted by unauthorized parties.

Internally, SSL works through a combination of asymmetric and symmetric encryption. When a client (e.g., a web browser) connects to a server (e.g., a website), they perform a handshake process to establish a secure connection. During this handshake, the server sends its public key to the client, which is used to encrypt data. The client then generates a symmetric encryption key, encrypts it with the server's public key, and sends it back to the server. Both the client and server now have a shared symmetric key, which they use for encrypted communication.

Once the secure connection is established, data transmitted between the client and server is encrypted using the shared symmetric key, ensuring confidentiality. Additionally, SSL provides mechanisms for verifying the authenticity of the server (through digital certificates) and ensuring the integrity of transmitted data (through message digests and digital signatures).

57. What are the 3 standard streams in Linux can you redirect them?
ans:-
The three standard streams in Linux are:

Standard Input (stdin): Used for input to commands. By default, it reads input from the keyboard.
Standard Output (stdout): Used for normal output from commands. By default, it displays output on the terminal.
Standard Error (stderr): Used for error messages from commands. By default, it also displays output on the terminal.
These streams can be redirected using special characters:

>: Redirects stdout to a file, overwriting the file if it already exists.
>>: Redirects stdout to a file, appending the output to the end of the file.
<: Redirects stdin from a file, providing input to a command from the file.
2>: Redirects stderr to a file.
&> or 2>&1: Redirects stderr to the same destination as stdout


58. How would you make a bash script print verbose/p so you can debug it?
ans:-
#!/bin/bash

# Enable verbose output
set -x

# Your script commands here
echo "This is a debug message"

59. What is ARP? What does it do?
ans:-

ARP (Address Resolution Protocol) is a communication protocol used in computer networks to map IP addresses to MAC addresses. Its primary function is to resolve IP addresses (network layer addresses) to MAC addresses (data link layer addresses) in local area networks (LANs).

When a device wants to communicate with another device on the same network, it needs to know the MAC address of the destination device. ARP helps in this process by sending out ARP requests to all devices on the network, asking "Who has this IP address?" The device with the matching IP address responds with its MAC address, allowing the requesting device to build an ARP table (also known as an ARP cache) containing the mappings of IP addresses to MAC addresses.

Once the ARP table is populated, subsequent communication within the network can occur efficiently, as devices no longer need to broadcast ARP requests for every packet sent. ARP also supports dynamic updating of the ARP table to adapt to changes in the network topology.







60. What is Nat? Uses?
ans:-
NAT (Network Address Translation) is a process used in networking to modify network address information in packet headers while in transit across a traffic routing device. Its primary uses include:

Conserving IP Addresses: NAT enables multiple devices on a local network to share a single public IP address, conserving the limited supply of IPv4 addresses.

Security: NAT acts as a barrier between the public internet and private local networks, hiding internal IP addresses from external networks and providing a level of security.

Load Balancing: NAT can distribute incoming traffic across multiple servers or devices by translating the destination IP address in packets to different internal IP addresses.

IP Version Transition: NAT can facilitate the transition from IPv4 to IPv6 by enabling IPv6-only networks to communicate with IPv4 networks and vice versa.

Overall, NAT plays a crucial role in managing network traffic, optimizing resource utilization, and enhancing network security.

61. What are VLANs & uses?
ans:-
VLANs (Virtual Local Area Networks) are a method of creating multiple logical networks within a single physical network infrastructure. Each VLAN operates as a separate broadcast domain, allowing devices within the same VLAN to communicate with each other as if they were on the same physical network, even if they are physically dispersed across different switches.

Uses of VLANs include:

Network Segmentation: VLANs help divide a large network into smaller, more manageable segments, improving network performance, security, and manageability.

Security: VLANs can isolate sensitive or critical systems from other parts of the network, reducing the risk of unauthorized access or attacks.

Broadcast Control: By limiting the scope of broadcast traffic, VLANs help reduce network congestion and improve overall network performance.

Quality of Service (QoS): VLANs can be used to prioritize network traffic based on VLAN membership, ensuring that critical applications receive adequate bandwidth and performance.

Flexibility: VLANs enable dynamic reconfiguration of network resources without the need for physical rewiring, facilitating changes in network topology, organization, or service deployment.

Overall, VLANs provide a flexible and scalable solution for network design and management, allowing organizations to tailor their network infrastructure to meet specific requirements and optimize performance.




62. What is VPN & Uses?
ans:-
VPN (Virtual Private Network) is a technology that allows users to establish a secure and encrypted connection over a public network, such as the internet. It creates a private tunnel between the user's device and a VPN server, encrypting all data transmitted between them and ensuring privacy and confidentiality.

Uses of VPN include:

Remote Access: VPNs enable remote workers to securely access corporate networks and resources from anywhere, providing a secure connection to company resources such as files, applications, and intranet sites.

Privacy and Anonymity: VPNs mask the user's IP address and encrypt their internet traffic, protecting their online privacy and preventing third parties from monitoring or intercepting their communication.

Bypassing Geographic Restrictions: VPNs allow users to bypass geographic restrictions and access region-restricted content, such as streaming services or websites, by connecting to servers in different locations around the world.

Enhanced Security: VPNs provide an additional layer of security when using public Wi-Fi networks, protecting against potential threats such as hacking, eavesdropping, and man-in-the-middle attacks.

Secure File Sharing: VPNs facilitate secure file sharing and collaboration among remote users, ensuring that sensitive data is transmitted securely and preventing unauthorized access to shared files and documents.

Overall, VPNs offer a versatile and powerful solution for securing and enhancing communication, collaboration, and access to resources in today's interconnected world

63. What is DNS? Uses?
ans:-
DNS (Domain Name System) is a hierarchical and distributed naming system used to translate domain names (e.g., www.example.com) into IP addresses (e.g., 192.0.2.1) and vice versa.

Uses of DNS include:

Hostname Resolution: DNS resolves domain names to IP addresses, allowing users to access websites and services using human-readable domain names instead of numerical IP addresses.

Email Routing: DNS is used to identify the mail server associated with a domain name, enabling email clients to deliver messages to the correct destination.

Load Balancing: DNS can distribute incoming network traffic across multiple servers or data centers based on factors such as server availability, proximity, or load, improving performance and reliability.

Service Discovery: DNS is used in conjunction with service discovery protocols to locate and connect to services and resources within a network or cloud environment.

Reverse DNS Lookup: DNS provides reverse mapping functionality, allowing IP addresses to be translated back into domain names, which is useful for troubleshooting, security analysis, and logging.

Overall, DNS plays a critical role in the functioning of the internet and enables seamless communication and connectivity between devices and services across the globe


64. What are TCP & UDP and the difference between them?
ans:-
TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) are both transport layer protocols used for communication over networks.

TCP:
Connection-oriented protocol.
Provides reliable, ordered, and error-checked delivery of data.
Establishes a connection before transmitting data and ensures data integrity through mechanisms such as acknowledgment, retransmission, and flow control.
Suitable for applications where data integrity and reliability are critical, such as web browsing, email, and file transfer.

UDP:
Connectionless protocol.
Provides unreliable, unordered, and unchecked delivery of data.
Does not establish a connection before transmitting data and does not guarantee delivery or order of packets.
Typically used for real-time applications such as streaming media, online gaming, VoIP, and DNS, where low latency and speed are prioritized over reliability.
In summary, TCP provides reliable, connection-oriented communication, while UDP offers faster, connectionless communication with lower overhead, making each protocol suitable for different types of applications based on their specific requirements.

65. What is DevOps?
ans:-
DevOps is a cultural and organizational approach that aims to improve collaboration and communication between development (Dev) and operations (Ops) teams throughout the software development lifecycle. It emphasizes automation, continuous integration, continuous delivery, and rapid feedback loops to deliver software faster, more reliably, and with higher quality. DevOps seeks to break down silos between teams, streamline processes, and foster a culture of shared responsibility for delivering and maintaining software applications.












66. What are Agile and waterfall methods?
ans:-
Agile and waterfall are two different software development methodologies:

Waterfall:
Sequential and linear approach.
Progresses through predefined stages: requirements, design, implementation, testing, deployment, and maintenance.
Each stage must be completed before moving to the next.
Changes are difficult and costly to implement once the project is underway.
Suitable for projects with well-defined requirements and stable environments.

Agile:
Iterative and incremental approach.
Emphasizes flexibility, collaboration, and customer feedback.
Breaks the project into small, manageable iterations called sprints.
Allows for changes and adjustments throughout the development process.
Promotes continuous improvement and adaptation to changing requirements.
Suitable for projects with evolving or unclear requirements, where rapid delivery and frequent feedback are essential


67. What is git lifecycle ?
ans:-
The Git lifecycle typically involves the following stages:

Working Directory: The directory on your local machine where you create, edit, and delete files as part of your development work.

Staging Area (Index): A temporary storage area where changes to files are prepared to be committed to the repository. Files in the staging area are ready to be included in the next commit.

Local Repository: The .git directory in your project folder, where Git stores the committed snapshots of your project's files. The local repository contains the complete history of changes made to your project.

Remote Repository: A separate repository hosted on a remote server (e.g., GitHub, GitLab) where you can push your local changes and collaborate with others. The remote repository serves as a central location for sharing and managing project files across multiple developers.

Committing: The process of recording changes to files in the local repository. Commits create a snapshot of the project at a specific point in time, along with a commit message describing the changes made.

Pushing: The process of uploading local commits to a remote repository, keeping the two repositories synchronized. Pushing allows you to share your changes with others and collaborate on a shared codebase.

Pulling: The process of downloading changes from a remote repository to update your local repository. Pulling allows you to incorporate changes made by others into your own project.

Overall, the Git lifecycle revolves around making changes to files in the working directory, staging those changes for commit, committing them to the local repository, and then optionally pushing them to a remote repository for collaboration with others


68. What is automation in DevOps?
ans:-
Automation in DevOps refers to the use of tools, scripts, and processes to automate repetitive tasks and workflows throughout the software development lifecycle. This includes tasks such as code compilation, testing, deployment, provisioning infrastructure, configuration management, and monitoring. By automating these tasks, DevOps teams can streamline processes, reduce manual effort, improve consistency, and accelerate delivery of software updates, leading to faster time-to-market, increased efficiency, and higher-quality software.


69. What is CI/CD?
ans:-
CI/CD stands for Continuous Integration and Continuous Delivery (or Continuous Deployment).

Continuous Integration (CI): The practice of frequently integrating code changes into a shared repository, where automated builds and tests are run to detect and address integration issues early in the development process. This ensures that changes are integrated smoothly and reduces the risk of integration conflicts.

Continuous Delivery (CD): The practice of automating the release process to ensure that software can be reliably and consistently delivered to production at any time. This includes automating deployment, testing, and validation processes, enabling rapid and reliable delivery of new features, fixes, and updates to users.

Overall, CI/CD practices promote collaboration, automation, and agility in software development, enabling teams to deliver high-quality software faster and more efficiently


70. What are the stages in Jenkins ?
ans:-
In Jenkins, the stages typically involved in a pipeline include:

Checkout: Fetching the source code from the version control system (e.g., Git, SVN).

Build: Compiling the source code, running tests, and generating artifacts.

Test: Executing automated tests to verify the functionality and quality of the software.

Deploy: Deploying the built artifacts to the target environment (e.g., development, staging, production).

Release: Optionally, performing additional tasks such as tagging the release, updating documentation, or notifying stakeholders.

These stages may vary depending on the specific requirements and workflow of the project. Jenkins allows users to define and customize pipelines to suit their needs


71. What is Ansible? Why is it used?
ans :-
Ansible is an open-source automation tool used for configuration management, application deployment, and task automation. It simplifies the management of large-scale IT environments by automating repetitive tasks and ensuring consistency across systems.

Why it is used:

Ease of Use: Uses a simple, human-readable language (YAML) for configuration, making it accessible to users with varying levels of expertise.
Agentless: Does not require any agent software to be installed on target machines, reducing overhead and simplifying maintenance.
Scalability: Manages large-scale deployments efficiently, making it suitable for both small and enterprise environments.
Flexibility: Supports a wide range of modules and integrations, allowing automation of various tasks across different platforms and technologies.
Consistency: Ensures consistent configuration and state across multiple systems, reducing the likelihood of configuration drift and errors.
Ansible is used to automate infrastructure provisioning, configuration management, application deployment, and other repetitive IT tasks, improving efficiency and reliability.


72. What is the build trigger?
ans:-
A build trigger in Jenkins is a mechanism that initiates a build of a project. Common build triggers include:

Poll SCM: Jenkins periodically checks the source code repository for changes.
Webhook: The version control system (e.g., GitHub, GitLab) sends a push notification to Jenkins when changes are made.
Schedule: Builds are triggered at specified intervals using cron-like syntax.
Manual: Users manually trigger a build from the Jenkins interface.
Build Completion: A build is triggered after the completion of another build.
These triggers ensure that the continuous integration process runs automatically based on specified conditions


73. What is POLL SCM?
ans:-
POLL SCM is a Jenkins build trigger that periodically checks the source code repository for changes. If changes are detected since the last build, Jenkins triggers a new build. This is useful for integrating changes automatically without requiring manual intervention or webhooks. The polling interval is configured using cron-like syntax.








74. What are the plugins used in the project?
ans:-
Here are some commonly used Jenkins plugins in a typical project setup:

Git Plugin: Integrates Jenkins with Git repositories.
Pipeline Plugin: Enables the use of Jenkins Pipeline as code.
SonarQube Plugin: Integrates SonarQube for static code analysis.
Slack Notification Plugin: Sends build notifications to Slack channels.
Docker Plugin: Provides Jenkins integration with Docker.
Build Timestamp Plugin: Adds timestamps to build logs.
Credentials Plugin: Manages credentials securely.
JUnit Plugin: Processes JUnit test results and integrates them with Jenkins builds.
Artifact Repository Plugin: Integrates with artifact repositories like Nexus or Artifactory.
Email Extension Plugin: Sends email notifications for build results.
These plugins enhance Jenkins functionality and streamline the CI/CD process

75. What are the ansible modules?
ans:-
Ansible provides a wide range of modules for automating various tasks across different systems and technologies. Some common Ansible modules include:

Package: Manages packages on the system (e.g., apt, yum).
File: Manages files and directories on the system.
Service: Controls system services (e.g., start, stop, restart).
User: Manages user accounts and groups on the system.
Command: Executes arbitrary commands on the remote system.
Copy: Copies files or directories from the local system to the remote system.
Template: Manages text files using Jinja2 templates.
Shell: Executes shell commands on the remote system.
Setup: Gathers facts about remote hosts (e.g., hardware, operating system).
Gathering: Controls fact gathering behavior.
Wait_for: Waits for a condition on the remote system to be satisfied.
These are just a few examples, and there are many more modules available for various use cases and integrations.

76. Day to Day activities for a DevOps ?
Ans:-
1. Make sure that the pipeline is running smoothly

 This is one of the most important task of a DevOps engineer to make sure that CI/CD pipeline is intact and fixing any issue or failure with it is the #1 priority for the day. They often need to spend time on troubleshooting, analysing and providing fixes to issues.


2. Interaction with other teams – Co-ordination and collaboration is the key for DevOps to be successful and hence daily integration with Dev and QA team, Program management, IT is always required.


3. Work on Automation Backlog – Automation is soul of DevOps so DevOps engineering need to plan it out and I can see DevOps engineer spending lots of time behind the keyboard working on Automating stuff on daily basis.


4. Infrastructure Management – DevOps engineer are also responsible for maintaining and managing the infrastructure required for CI/CD pipeline and making sure that its up and running and being used optimally is also part of their daily schedule. Working on Backup, High Availability, New Platform setup etc.


5. Dealing with Legacy stuff – Not everyone is lucky to work on latest and newest things and DevOps engineers are no exception hence they also need to spend time on legacy i.e. in terms of supporting it or migrating to the latest.


6. Exploration – DevOps leverage a lot from the various tools which are available, there are many options as open source so team need to regularly check on this to make sure the adoptions as required, this is something which also require some effort not on daily but regular basis. What are open source options available to keep the cost at minimum?


7. Removing bottleneck – DevOps primary purpose is identify the bottlenecks / Manual handshakes and work with everyone involved (Dev / QA and all other stakeholder) to remove them so team spend good amount of time in finding such things and build the Automation Backlog using this.

8. Documentation – Though Agile / DevOps stresses less on the documentation, it is still the important one which DevOps engineer does on daily basis, Be it Server Information, Daily Week charted, Scrum / Kanban board or Simple steps to configure / backup or modify the infrastructure, you need to spent good amount of time in coming up these artifacts.


9. Training and Self Development – Self leaning and Training is very useful in getting better understanding and many organisations encourage their employee to take the time out and do some of these and same holds true for DevOps folks as well, So learn something new everyday.


10. Continuous Improvement as Practice – Last but not least, It’s up to the DevOps folks to build awareness on the potential of CI/CD and DevOps practices and building a culture of leveraging it for doing things better, reducing re-work, increasing the productivity and optimising the use of existing resources

77. What do you mean by Git hotfix ?
ans:-
In the context of Git and software development, a "hotfix" is a type of urgent update meant to address a critical issue in the software, such as a bug or security vulnerability, that needs to be resolved immediately without waiting for the next scheduled release. Here’s a breakdown of how hotfixes are typically managed in a Git workflow:

Identify the Issue: A critical bug or issue is identified that requires an immediate fix.

Create a Hotfix Branch: From the main or master branch (or whatever the stable production branch is called), create a new branch specifically for the hotfix. This is often named something like hotfix-issue-description or hotfix-1234 where 1234 might be the issue number.


git checkout main
git pull origin main
git checkout -b hotfix-1234
Implement the Fix: Make the necessary changes in the hotfix branch to resolve the issue.

Commit the Changes: Commit the changes with a descriptive message.

git add .
git commit -m "Fix critical bug causing issue #1234"
Test the Hotfix: Ensure the hotfix is properly tested to verify that it resolves the issue without introducing new problems.

Merge the Hotfix: Once the hotfix is confirmed to be working, merge it back into the main or master branch.



git checkout main
git pull origin main
git merge hotfix-1234
Tag the Hotfix (Optional): It's common to tag the hotfix release for easy reference.


git tag -a v1.0.1 -m "Hotfix for issue #1234"
git push origin --tags
Deploy the Hotfix: Deploy the updated main branch to the production environment.

Merge the Hotfix into Other Branches: If you have other long-lived branches like develop (in a Git Flow setup), you should also merge the hotfix into these branches to ensure the fix is incorporated into future releases.


git checkout develop
git pull origin develop
git merge main
Delete the Hotfix Branch: Once the hotfix has been merged and deployed, you can delete the hotfix branch to keep the repository clean.


git branch -d hotfix-1234
git push origin --delete hotfix-1234
This process ensures that critical issues are addressed swiftly while maintaining a clean and organized Git history.


78.  If you pull code from github but unfortunately code if failing how to roll back to previous code without pulling the code from git hub ?
Ans:-
By using Git reset command 
ex:- git reset –hard commithash

79.  How to create folder inside another folder recursively ?
Ans :- 
mkdir -p Folder/childfolder1/childfolder2


80. Difference between docker compose and docker ?
Ans:-
Key Differences:
    • Scope: Docker focuses on container management at the individual container level, while Docker Compose is designed for managing multi-container applications.
    • Functionality: Docker provides core containerization functionality such as building and running containers, whereas Docker Compose adds orchestration capabilities for defining and managing multi-container applications.
    • Workflow: Docker is typically used for building and running containers in development, testing, and production environments, while Docker Compose is primarily used in development and testing environments to simplify the management of multi-container applications.
81.  what is git stash ?
Ans:-
Git stash is a command used to temporarily store changes that are not ready to be committed. It saves modifications and reverts the working directory to its clean state, allowing you to switch branches or perform other operations without committing incomplete changes.

82.  What is Vault in ansible ?
Ans:-
In Ansible, Vault is a feature that allows you to securely store and manage sensitive data such as passwords, keys, and confidential information. It encrypts this data, ensuring that it remains protected while still being accessible to your playbooks and roles when needed.

83. What is ad-hoc command in ansible ?
Ans:- 
An ad-hoc command in Ansible is a one-time, quick command executed directly on remote hosts without the need to write a playbook. It allows you to perform tasks such as running shell commands, managing packages, or restarting services on multiple hosts from the command line.

84. what is roles in ansible ?
Ans:-
In Ansible, roles are a way to organize playbooks and reusable tasks into structured, modular components. Roles enable you to separate configuration into independent, reusable units, making complex playbooks easier to manage and share. Each role can include variables, tasks, handlers, templates, files, and dependencies, all contained within a directory structure. This helps maintain clean, organized, and scalable configurations.

85. what is ansible galaxy ?
Ans:-
Ansible Galaxy is a repository and community hub for sharing, discovering, and downloading Ansible roles and collections. It allows users to find pre-built roles and collections created by the community or organizations, which can be used to automate various tasks and configurations. You can also publish your own roles and collections to Galaxy, making them available for others to use. Ansible Galaxy simplifies the reuse of automation code and promotes best practices.



86. what is task in ansible ?
Ans:-
In Ansible, a task is a single action or operation defined in a playbook to be executed on managed nodes. Tasks are the basic units of work and can include actions such as running commands, installing packages, copying files, and managing services. Each task uses a specific Ansible module to perform its function and is executed in the order they appear in the playbook. Tasks help automate system configurations and deployments.

87. what is modules in ansible ?
Ans:-
In Ansible, modules are reusable, standalone scripts or programs that perform specific tasks on managed nodes. They are the building blocks used in playbooks and ad-hoc commands to automate tasks such as installing packages, managing files, executing commands, and configuring services. Ansible includes a wide range of built-in modules, and you can also create custom modules to extend Ansible's functionality.

88. Git merge hooks ?
Ans:-
Git merge hooks are scripts that Git executes automatically during the merge process. They allow you to customize or validate the merge operation by performing certain actions before or after the merge.

For example, you can use a merge hook to enforce coding standards, run tests, or perform any other checks specific to your project before allowing a merge to proceed.

These hooks are stored in the .git/hooks directory of your Git repository and can be written in any scripting language (e.g., Bash, Python). Git provides several merge hooks such as pre-merge-commit, prepare-commit-msg, and post-merge, among others.

To use a merge hook, you need to create or modify the appropriate script in the .git/hooks directory with the desired functionality. Ensure that the script is executable (chmod +x) to allow Git to run it during the merge process.

89. git patch when we use it ?
Ans:-
Git patch is used to share changes between repositories or collaborators, especially for code reviews, bug fixes, or contributions to open-source projects.

90. A company wants to create thousands of Containers. Is there a limit on how many containers you can run in Docker?
Ans:-
The amount of containers that may be run under Docker has no explicitly specified limit. But it all relies on the constraints, particularly the hardware constraints. The size of the program and the number of CPU resources available are two major determinants of this restriction. If your program isn't too large and you have plenty of CPU resources, we can run a lot of containers.






91. Can you explain how Docker container differs from virtual machines?
Ans:-
Docker containers share the host OS kernel, making them lightweight and efficient. They provide process-level isolation and package applications with their dependencies.Virtual machines, in contrast, run full guest OSs on a hypervisor, consuming more resources and offering stronger isolation



92. You're in charge of maintaining Docker environments in your company and You've noticed many stopped containers and unused networks taking up space. Describe how you would clean up these resources effectively to optimize the Docker environment ?
Ans:-
The docker prune command is used to clean up unused Docker resources, such as containers, volumes, networks, and images. It helps reclaim disk space and tidy up the Docker environment by removing objects that are not in use.
There are different types of docker prune commands:
    • docker container prune: Removes stopped containers. 
    • docker volume prune: Deletes unused volumes. 
    • docker network prune: Cleans up unused networks. 
    • docker image prune: Removes unused images. 
Running docker system prune combines these functionalities into one command, ensuring that Docker removes any resources not associated with a running container.



93. You're managing a Docker environment and need to ensure that each container operates within defined CPU and memory limits. How do you limit the CPU and memory usage of a Docker container?
Ans:-
Docker allows you to limit the CPU and memory usage of a container using resource constraints. You can set the CPU limit with the --cpu option and the memory limit with the --memory option when running the container using the docker run command.
For example, docker run --cpu 2 --memory 1g mycontainer limits the container to use a maximum of 2 CPU cores and 1GB of memory.

94. You've been tasked with ensuring the application can handle increased loads by scaling Docker containers horizontally. How do you scale Docker containers horizontally?
Ans:-
To scale Docker containers horizontally, you can use Docker Swarm or a container orchestration tool like Kubernetes.
Which lets you create and manage multiple docker containers defining the number of replicas in declarative way. (Manifests)

95. What is the difference between a Docker container and a Kubernetes pod?
Ans:-
A Docker container is a lightweight and isolated runtime environment that runs a single instance of an application. It is managed by Docker and provides process-level isolation.
On the other hand, a Kubernetes pod is a higher-level abstraction that can contain one or more Docker containers (or other container runtimes).

96. Can you describe a situation where you optimized a Dockerfile for faster build times or smaller image size?
Ans:-
Optimizing a Dockerfile could involve various strategies like using a smaller base image, reducing the number of layers by combining commands, or using multi-stage builds to exclude unnecessary files from the final image.





97. You're part of a development team deploying a microservices architecture using Docker containers. One of the containers, critical to the system's functionality, has suddenly started failing without clear error messages. How do you debug issues in a failing Docker container ?
Ans:-
There are several techniques to debug issues in a Docker container:
Logging: Docker captures the standard output and standard error streams of containers, making it easy to inspect logs using the docker logs command.
Shell access: You can access a running container's shell using the docker exec command with the -it option. This allows you to investigate and troubleshoot issues interactively.
Image inspection: You can inspect the Docker image's contents and configuration using docker image inspect. This lets you check for potential misconfigurations or missing dependencies.
Health checks: Docker supports defining health checks for containers, allowing you to monitor the health status and automatically restart or take action based on predefined conditions.

98. You're working on a critical application running in Docker containers, and an update needs to be applied without risking data loss. How do you update a Docker container without losing data?
Ans:-
The steps to update a Docker container without losing data are:
    • Create a backup of any important data stored within the container. 
    • Stop the container gracefully using the docker stop command. 
    • Pull the latest version of the container image using docker pull. 
    • Start a new container using the updated image, making sure to map any necessary volumes or bind mounts. 
    • Verify that the new container is functioning correctly and that the data is still intact. 
      
99. You're responsible for securing the Docker containers hosting your organization's sensitive applications. How do you secure Docker containers ?
Ans:-
Securing Docker containers involves implementing a multi-layered approach. Some recommended practices include:
    • Using trusted base images from reputable sources. 
    • Regularly updating Docker and the underlying host system with security patches. 
    • Scanning container images for vulnerabilities using security tools like docker scout. 
    • Implementing network segmentation and access controls. 
    • Monitoring container activity and logging for security analysis. 
    • Applying the least privilege principle and implementing container-specific security measures like AppArmor or seccomp profiles. 
100. How do you monitor Docker containers?
Ans:-
There are various ways to monitor Docker containers, including:
    • Using Docker's built-in container monitoring commands, such as docker stats and docker container stats, to view resource usage statistics. 
    • Integrating with container monitoring and logging tools like Prometheus, Grafana, or ELK stack (Elasticsearch, Logstash, Kibana) to collect and analyze container metrics and logs. 
    • Leveraging container orchestration platforms that offer built-in monitoring capabilities such as Docker Swarm's service metrics or Kubernetes' metrics API. 
    • Using specialized monitoring agents or tools that provide container-level insights and integration with broader monitoring and alerting systems. 























101. What is JNLP and why is it used in Jenkins ?
Ans:
 In Jenkins, JNLP is used to allow agents (also known as "slave nodes") to be launched and managed remotely by the Jenkins master instance. This allows Jenkins to distribute build tasks to multiple agents, providing scalability and improving performance.
When a Jenkins agent is launched using JNLP, it connects to the Jenkins master and receives build tasks, which it then executes. The results of the build are then sent back to the master and displayed in the Jenkins user interface.
102. You have multiple web servers running Apache, and you need to ensure that a specific configuration file is consistent across all servers. How would you achieve this with Ansible? Ans:-To ensure a specific configuration file is consistent across multiple Apache web servers using Ansible, you can use the copy module to distribute the file and ensure it's the same on all servers.







103. Your team is deploying a new version of a web application across multiple servers. How would you use Ansible to automate the deployment process?
Ans:-
To automate the deployment of a new web application version across multiple servers using Ansible, follow these steps:
    1. Inventory File: List all target servers in an inventory file.
    2. Ansible Playbook: Create a playbook to:
        ◦ Pull the latest code from the repository.
        ◦ Install dependencies.
        ◦ Configure the application.
        ◦ Restart the necessary services.

                                                                                                                                                                





104. You want to perform rolling updates to a group of servers without causing downtime. How would you implement this using Ansible ?
Ans:-
To perform rolling updates to a group of servers without causing downtime using Ansible, use the serial keyword to update a limited number of servers at a time.
                
Key Steps
    1. Define Inventory: List target servers in the inventory file.
    2. Configure Serial: Use serial: 1 to update one server at a time.

105.  What are the various way to verify playbooks prior to execution ?
Ans:-
-- check
-- diff
--syntax-check





106. How to continue Anisble playbooks tasks, even if some tasks fails ?
Ans:-
By defaults, Ansible stop executing tasks on a host when a task fails on that host 
but we can use ignore_errors keyword to continue despite failure 
example :-
---
- name: copyiing files
  hosts: all
  user: rahul.yadav@apmosys.mahape
  gather_facts: false
  tasks: 
    - name: copying files
      copy:
        src: /home/rahul.yadav@apmosys.mahape/sachin/index.html
        dest: /home/rahul.yadav@apmosys.mahape/pratyush/
     ignore_errors: true

107. Explain ways to run the same Ansible playbooks on distinctive nodes with different port ans usernames ?
Ans:-
by Adding below paramter on inventory file :-
ansible_port 
ansible_user

for example :-
192.168.6.162 ansible_user=apmosys anisble_ssh_pass = test@123 ansible_port=8443 

108. How do we trigger “Webserver” service restart task whenever there is a configuration change in “Webserver ” or else skip the restart task ?
Ans:-
We can use Handlers option for achieving above use case

109. How do we handle sensitive information like keys, passwords in ansible ?
Ans:-
By using Ansible Vault service

110. Difference between a Dockerfile & Docker compose file ?
Ans:- 

1. Dockerfile is a simple text file that contains command a user could call to assemble an image
1. Docker compose is a tool that allows you to define ans run multi-container docker applications
2. docker build -t imagename
2. docker compose-up


111. After successful executing the docker command “docker run ubuntu” ubuntu container gets killed automatically ? Kindly tell us the reason why?
Ans:-
    • that is the default behaviour of a container, whenever a process running inside the container exits, the container is killed/exits automatically.
    • If you want to container to run then make the ENTRYPOINT/CMD script run in the foreground without exiting.
    • 
112.  How do we run NGINX web server on localhost:8088 using a bridge network ?
Ans:- by using below command to run 
docker run -it -d -p 8088:80 –name webserver_nginx

113. How to make container volume persistent ? And how we mount the volume on the localhost “/root/nginx” directory ? Not in default directory “/var/lib/volumes” ?
Ans:- 
bu using bind mount option we can achieve this.

114. What is git fork & how it is used in github ?
Ans:- 
git fork is copy of repository that allow user to freely experiment with changes without affecting original repository
